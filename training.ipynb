{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUD/nUVyCXo2YWXB72ySIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SridharSola/FER/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZrpcRjj5fwZ"
      },
      "outputs": [],
      "source": [
        "def get_set_acc(loader, net):\n",
        "  if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "  test_acc = []\n",
        "  total_t=0\n",
        "  correct_t=0\n",
        "  net.eval() #Needed!!!!\n",
        "  for data_t, target_t in (loader):\n",
        "\n",
        "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
        "            outputs_t = net(data_t)\n",
        "            #print(outputs_t.shape)\n",
        "            _,pred_t = torch.max(outputs_t, dim=1)\n",
        "            correct_t += torch.sum(pred_t==target_t).item()\n",
        "            total_t += target_t.size(0)\n",
        "            test_acc.append(100 * correct_t/total_t) \n",
        "  print(f'test acc: {(100 * correct_t/total_t):.4f}\\n')\n",
        "\n",
        "def adjust_learning_rate(optimizer): \n",
        "  for param_group in optimizer.param_groups: \n",
        "      param_group[\"lr\"] /= 10.\n",
        "\n",
        "def accuracy(out, labels):\n",
        "    _,pred = torch.max(out, dim=1)\n",
        "    return torch.sum(pred==labels).item()\n",
        "\n",
        "def training(net, train_set, test_set, criterion, learning_rate, epochs, log, model_save):\n",
        "  if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "  criterion = criterion\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate) \n",
        "  logfile = open(log, 'w')\n",
        "  n_epochs = epochs\n",
        "  print_every = 5\n",
        "  test_loss_min = np.Inf\n",
        "  test_loss = []\n",
        "  test_acc = []\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  best_test_acc = 0.0\n",
        "  total_step = len(train_set)\n",
        "  lrs = []\n",
        "  lrs.append(learning_rate)\n",
        "  print('\\nTraining starting:\\n', file = logfile)\n",
        "  print('\\nTraining starting:\\n')\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "      running_loss = 0.0\n",
        "      correct = 0\n",
        "      total=0\n",
        "      print(f'Epoch {epoch}\\n', file = logfile)\n",
        "      print(f'Epoch {epoch}\\n')\n",
        "      if epoch == 20 or epoch == 28 or epoch==36:     \n",
        "          #scheduler.step()\n",
        "          adjust_learning_rate(optimizer)\n",
        "          lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "          print(f'Updated lr: {lrs[-1]}\\n', file = logfile)\n",
        "          print(f'Updated lr: {lrs[-1]}\\n')\n",
        "      for batch_idx, (data_, target_) in enumerate(train_set):\n",
        "          data_, target_ = data_.to(device), target_.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          outputs = net(data_)\n",
        "          loss = criterion(outputs, target_)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          #out, probs = cls(outputs)\n",
        "          \n",
        "          running_loss += loss.item()\n",
        "          _,pred = torch.max(outputs, dim=1)\n",
        "          correct += torch.sum(pred==target_).item()\n",
        "          total += target_.size(0)\n",
        "          if (batch_idx) % 20 == 0:\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                    .format(epoch, n_epochs, batch_idx, total_step, loss.item()), file = logfile)\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                    .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
        "      train_acc.append(100 * correct / total)\n",
        "      train_loss.append(running_loss/total_step)\n",
        "      print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}', file = logfile)\n",
        "      print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
        "      batch_loss = 0\n",
        "      total_t=0\n",
        "      correct_t=0\n",
        "      \n",
        "      with torch.no_grad():\n",
        "          net.eval()\n",
        "          for data_t, target_t in (test_set):\n",
        "              data_t, target_t = data_t.to(device), target_t.to(device)\n",
        "              outputs_t = net(data_t)\n",
        "              #outputs_t, probs_t = cls(outputs_t1)\n",
        "              loss_t = criterion(outputs_t, target_t)\n",
        "              batch_loss += loss_t.item()\n",
        "              _,pred_t = torch.max(outputs_t, dim=1)\n",
        "              correct_t += torch.sum(pred_t==target_t).item()\n",
        "              total_t += target_t.size(0)\n",
        "          test_acc.append(100 * correct_t/total_t)\n",
        "          test_loss.append(batch_loss/len(test_loader))\n",
        "          network_learned = batch_loss < test_loss_min\n",
        "          print(f'test loss: {np.mean(test_loss):.4f}, test acc: {(100 * correct_t/total_t):.4f}\\n', file = logfile)\n",
        "          print(f'test loss: {np.mean(test_loss):.4f}, test acc: {(100 * correct_t/total_t):.4f}\\n')\n",
        "\n",
        "          if best_test_acc < test_acc[-1]:\n",
        "            #Updating the best test_accuracy obtained\n",
        "            print(\"Best Test Accuracy Updated\", file = logfile)\n",
        "            print(\"Best Test Accuracy Updated\")\n",
        "            best_test_acc = test_acc[-1]\n",
        "          if network_learned:\n",
        "              test_loss_min = batch_loss\n",
        "              torch.save(net.state_dict(), model_save)\n",
        "              print('Improvement-Detected, save-model to drive', file = logfile)\n",
        "              print('Improvement-Detected, save-model to drive')\n",
        "\n",
        "\n",
        "      net.train()\n",
        "\n",
        "  print(\"The best test accuracy obtained was: \", best_test_acc)\n",
        "  #End of training\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "def get_conf_matrix(net, loader):\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  net.eval()\n",
        "  # iterate over test data\n",
        "  for inputs, labels in loader:\n",
        "          output = net(inputs) # Feed Network\n",
        "\n",
        "          output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
        "          y_pred.extend(output) # Save Prediction\n",
        "          \n",
        "          labels = labels.data.cpu().numpy()\n",
        "          y_true.extend(labels) # Save Truth\n",
        "\n",
        "  # constant for classes\n",
        "  classes = ('Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger', 'Neutral')\n",
        "\n",
        "  # Build confusion matrix\n",
        "  cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "  df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
        "                      columns = [i for i in classes])\n",
        "\n",
        "  df_conf_norm = df_cm / df_cm.sum(axis=1)\n",
        "  plt.figure(figsize = (12,7))\n",
        "  sn.heatmap(df_conf_norm, annot=True)\n",
        "  plt.savefig('output.png')\n",
        "\n"
      ]
    }
  ]
}